---
phase: 20-backend-statistics-and-ddl-api
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - lineage-api/python_server.py
autonomous: true

must_haves:
  truths:
    - "GET /api/v2/openlineage/datasets/{datasetId}/statistics returns row count, size, owner, dates, type for a known dataset via Python server"
    - "GET /api/v2/openlineage/datasets/{datasetId}/ddl returns view SQL, table comment, and column comments via Python server"
    - "Both Python endpoints return 404 for unknown dataset IDs"
    - "Internal errors return generic error message (no SQL or connection details leaked)"
    - "Both tables and views return valid responses (views have null size)"
  artifacts:
    - path: "lineage-api/python_server.py"
      provides: "Statistics and DDL Flask route handlers"
      contains: "get_dataset_statistics"
  key_links:
    - from: "lineage-api/python_server.py statistics endpoint"
      to: "DBC system views"
      via: "teradatasql cursor.execute"
      pattern: "DBC\\.TablesV|DBC\\.TableStatsV|DBC\\.TableSizeV"
    - from: "lineage-api/python_server.py ddl endpoint"
      to: "DBC system views"
      via: "teradatasql cursor.execute"
      pattern: "DBC\\.TablesV|DBC\\.ColumnsJQV"
---

<objective>
Add statistics and DDL API endpoints to the Python Flask server, matching the same API contract as the Go backend.

Purpose: The Python server is the primary development/testing server (recommended for testing per CLAUDE.md). Both servers must implement identical endpoints so the frontend works with either backend. This plan is independent of the Go backend plan and can execute in parallel.

Output: Two new Flask route handlers at GET /api/v2/openlineage/datasets/{datasetId}/statistics and GET /api/v2/openlineage/datasets/{datasetId}/ddl in python_server.py.
</objective>

<execution_context>
@/Users/Daniel.Tehan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/Daniel.Tehan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-backend-statistics-&-ddl-api/20-RESEARCH.md

@lineage-api/python_server.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add statistics and DDL endpoints to Python Flask server</name>
  <files>lineage-api/python_server.py</files>
  <action>
    Add two new Flask routes to python_server.py. Place them AFTER the existing `get_dataset` function (the route at `/api/v2/openlineage/datasets/<path:dataset_id>`) and BEFORE the `search_datasets` route. This maintains logical grouping of dataset-related endpoints.

    **1. Statistics endpoint:**

    ```python
    @app.route("/api/v2/openlineage/datasets/<path:dataset_id>/statistics", methods=["GET"])
    def get_dataset_statistics(dataset_id):
        """Get statistics for a dataset (table/view)."""
        try:
            with get_db_connection() as conn:
                with conn.cursor() as cur:
                    # Verify dataset exists in OL_DATASET
                    cur.execute("""
                        SELECT source_type FROM OL_DATASET WHERE dataset_id = ?
                    """, [dataset_id])
                    ds_row = cur.fetchone()
                    if not ds_row:
                        return jsonify({"error": "Dataset not found"}), 404

                    # Parse database.table from dataset name
                    # Format: "namespace_id/database.table"
                    name_part = dataset_id.split("/", 1)[1] if "/" in dataset_id else dataset_id
                    parts = name_part.split(".", 1)
                    if len(parts) != 2:
                        return jsonify({"error": "Dataset not found"}), 404
                    db_name, table_name = parts[0].strip(), parts[1].strip()

                    # Query DBC.TablesV for table/view metadata
                    cur.execute("""
                        SELECT
                            TRIM(t.TableName),
                            t.TableKind,
                            TRIM(t.CreatorName),
                            t.CreateTimeStamp,
                            t.LastAlterTimeStamp,
                            TRIM(t.CommentString)
                        FROM DBC.TablesV t
                        WHERE t.DatabaseName = ?
                          AND t.TableName = ?
                    """, [db_name, table_name])
                    tab_row = cur.fetchone()

                    if not tab_row:
                        return jsonify({"error": "Dataset not found"}), 404

                    table_kind = tab_row[1].strip() if tab_row[1] else ""
                    source_type = "VIEW" if table_kind == "V" else "TABLE"

                    result = {
                        "datasetId": dataset_id,
                        "databaseName": db_name,
                        "tableName": tab_row[0] if tab_row[0] else table_name,
                        "sourceType": source_type,
                        "creatorName": tab_row[2] if tab_row[2] else None,
                        "createTimestamp": tab_row[3].isoformat() if tab_row[3] else None,
                        "lastAlterTimestamp": tab_row[4].isoformat() if tab_row[4] else None,
                        "rowCount": None,
                        "sizeBytes": None,
                        "tableComment": tab_row[5] if tab_row[5] else None,
                    }

                    # Query DBC.TableStatsV for row count (may fail on permission)
                    try:
                        cur.execute("""
                            SELECT RowCount
                            FROM DBC.TableStatsV
                            WHERE DatabaseName = ? AND TableName = ?
                              AND IndexNumber = 1
                        """, [db_name, table_name])
                        stats_row = cur.fetchone()
                        if stats_row and stats_row[0] is not None:
                            result["rowCount"] = int(stats_row[0])
                    except Exception:
                        pass  # Permission or availability issue, leave rowCount null

                    # Query DBC.TableSizeV for size (only meaningful for tables, not views)
                    if source_type == "TABLE":
                        try:
                            cur.execute("""
                                SELECT SUM(CurrentPerm)
                                FROM DBC.TableSizeV
                                WHERE DatabaseName = ? AND TableName = ?
                            """, [db_name, table_name])
                            size_row = cur.fetchone()
                            if size_row and size_row[0] is not None:
                                result["sizeBytes"] = int(size_row[0])
                        except Exception:
                            pass  # Permission or availability issue, leave sizeBytes null

            return jsonify(result)
        except Exception as e:
            import traceback
            traceback.print_exc()
            return jsonify({"error": "Internal server error"}), 500
    ```

    IMPORTANT: The outer except block must return "Internal server error" (not `str(e)`) to match API-05 security requirement. This differs from some existing Python endpoints that return `str(e)` -- the new endpoints must NOT leak error details.

    **2. DDL endpoint:**

    ```python
    @app.route("/api/v2/openlineage/datasets/<path:dataset_id>/ddl", methods=["GET"])
    def get_dataset_ddl(dataset_id):
        """Get DDL/definition for a dataset (table/view)."""
        try:
            with get_db_connection() as conn:
                with conn.cursor() as cur:
                    # Verify dataset exists in OL_DATASET
                    cur.execute("""
                        SELECT source_type FROM OL_DATASET WHERE dataset_id = ?
                    """, [dataset_id])
                    ds_row = cur.fetchone()
                    if not ds_row:
                        return jsonify({"error": "Dataset not found"}), 404

                    # Parse database.table from dataset name
                    name_part = dataset_id.split("/", 1)[1] if "/" in dataset_id else dataset_id
                    parts = name_part.split(".", 1)
                    if len(parts) != 2:
                        return jsonify({"error": "Dataset not found"}), 404
                    db_name, table_name = parts[0].strip(), parts[1].strip()

                    # Query DBC.TablesV for view SQL and table comment
                    # Try with RequestTxtOverFlow first, fall back without it
                    truncated = False
                    view_sql = None
                    table_comment = None
                    table_kind = None

                    try:
                        cur.execute("""
                            SELECT
                                t.TableKind,
                                TRIM(t.CommentString),
                                t.RequestText,
                                t.RequestTxtOverFlow
                            FROM DBC.TablesV t
                            WHERE t.DatabaseName = ?
                              AND t.TableName = ?
                        """, [db_name, table_name])
                        tab_row = cur.fetchone()
                        if tab_row:
                            table_kind = tab_row[0].strip() if tab_row[0] else ""
                            table_comment = tab_row[1] if tab_row[1] else None
                            if tab_row[2]:
                                view_sql = tab_row[2].strip() if isinstance(tab_row[2], str) else str(tab_row[2]).strip()
                            truncated = tab_row[3] == "Y" if tab_row[3] else False
                    except Exception:
                        # RequestTxtOverFlow column may not exist, retry without it
                        cur.execute("""
                            SELECT
                                t.TableKind,
                                TRIM(t.CommentString),
                                t.RequestText
                            FROM DBC.TablesV t
                            WHERE t.DatabaseName = ?
                              AND t.TableName = ?
                        """, [db_name, table_name])
                        tab_row = cur.fetchone()
                        if tab_row:
                            table_kind = tab_row[0].strip() if tab_row[0] else ""
                            table_comment = tab_row[1] if tab_row[1] else None
                            if tab_row[2]:
                                view_sql = tab_row[2].strip() if isinstance(tab_row[2], str) else str(tab_row[2]).strip()
                                truncated = len(view_sql) >= 12500

                    if table_kind is None:
                        return jsonify({"error": "Dataset not found"}), 404

                    source_type = "VIEW" if table_kind == "V" else "TABLE"

                    # Only set viewSql for views
                    if source_type != "VIEW":
                        view_sql = None
                        truncated = False

                    result = {
                        "datasetId": dataset_id,
                        "databaseName": db_name,
                        "tableName": table_name,
                        "sourceType": source_type,
                        "viewSql": view_sql,
                        "truncated": truncated,
                        "tableComment": table_comment,
                        "columnComments": {},
                    }

                    # Query DBC.ColumnsJQV for column comments
                    try:
                        cur.execute("""
                            SELECT TRIM(ColumnName), TRIM(CommentString)
                            FROM DBC.ColumnsJQV
                            WHERE DatabaseName = ?
                              AND TableName = ?
                              AND CommentString IS NOT NULL
                              AND TRIM(CommentString) <> ''
                            ORDER BY ColumnId
                        """, [db_name, table_name])
                        for row in cur.fetchall():
                            col_name = row[0].strip() if row[0] else ""
                            col_comment = row[1].strip() if row[1] else ""
                            if col_name and col_comment:
                                result["columnComments"][col_name] = col_comment
                    except Exception:
                        pass  # Permission issue, return empty column comments

            return jsonify(result)
        except Exception as e:
            import traceback
            traceback.print_exc()
            return jsonify({"error": "Internal server error"}), 500
    ```

    **Route ordering:** Flask uses `<path:dataset_id>` which captures everything including slashes. The more specific routes (`/statistics`, `/ddl`) must be registered BEFORE the generic `/<path:dataset_id>` route. However, Flask matches routes in registration order, and since `/statistics` and `/ddl` are suffixes of the datasetId path, Flask should handle this correctly as long as the routes are registered with the full path pattern.

    Actually, re-reading the existing code: the existing route is `@app.route("/api/v2/openlineage/datasets/<path:dataset_id>", methods=["GET"])`. The new routes are `/api/v2/openlineage/datasets/<path:dataset_id>/statistics` and `/api/v2/openlineage/datasets/<path:dataset_id>/ddl`. Flask will match the more specific path (with /statistics or /ddl suffix) before the generic one because the more specific routes have additional fixed segments after the path parameter.

    Place the new route functions AFTER the existing `get_dataset` function and BEFORE `search_datasets`.
  </action>
  <verify>
    Run `cd /Users/Daniel.Tehan/Code/lineage/lineage-api && python -c "import python_server; print('Import OK')"` -- must import without errors (validates syntax).
    Grep for `"Internal server error"` in both new functions to confirm security-compliant error handling.
    Grep for `get_dataset_statistics` and `get_dataset_ddl` function definitions exist.
  </verify>
  <done>
    - Statistics endpoint at /api/v2/openlineage/datasets/{datasetId}/statistics returns JSON with datasetId, databaseName, tableName, sourceType, creatorName, createTimestamp, lastAlterTimestamp, rowCount, sizeBytes, tableComment
    - DDL endpoint at /api/v2/openlineage/datasets/{datasetId}/ddl returns JSON with datasetId, databaseName, tableName, sourceType, viewSql, truncated, tableComment, columnComments
    - Both return 404 for missing datasets (checked against OL_DATASET first)
    - Both return generic "Internal server error" on 500 (no str(e) leak)
    - DBC queries wrapped in individual try/except to handle permission issues gracefully
    - python_server.py imports cleanly
  </done>
</task>

</tasks>

<verification>
1. `cd /Users/Daniel.Tehan/Code/lineage/lineage-api && python -c "import python_server; print('OK')"` succeeds
2. Grep for `def get_dataset_statistics` in python_server.py confirms function exists
3. Grep for `def get_dataset_ddl` in python_server.py confirms function exists
4. Grep for `"Internal server error"` in both new endpoints (not `str(e)`)
5. Grep for `DBC.TablesV` in python_server.py confirms DBC queries present
6. Grep for `DBC.ColumnsJQV` in python_server.py confirms column comment query
</verification>

<success_criteria>
- Python server imports cleanly with new endpoints
- Statistics endpoint queries DBC.TablesV + DBC.TableStatsV + DBC.TableSizeV
- DDL endpoint queries DBC.TablesV + DBC.ColumnsJQV
- Both verify dataset exists in OL_DATASET before querying DBC
- Error handling uses generic messages (API-05 security)
- 404 for missing datasets
- Views return null sizeBytes, tables return null viewSql
</success_criteria>

<output>
After completion, create `.planning/phases/20-backend-statistics-&-ddl-api/20-02-SUMMARY.md`
</output>
