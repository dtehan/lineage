---
phase: 17-cte-performance-optimization
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - database/benchmark_cte.py
  - database/benchmark_results.md
autonomous: true

must_haves:
  truths:
    - "Baseline benchmarks documented for depths 5, 10, 15, 20"
    - "Query execution times measured and recorded"
    - "Row counts at each depth documented"
    - "Performance bottlenecks identified via EXPLAIN analysis"
  artifacts:
    - path: "database/benchmark_cte.py"
      provides: "CTE performance benchmark suite"
      min_lines: 150
    - path: "database/benchmark_results.md"
      provides: "Baseline benchmark results and analysis"
      contains: "| Depth | Time"
  key_links:
    - from: "database/benchmark_cte.py"
      to: "database/db_config.py"
      via: "import CONFIG"
      pattern: "from db_config import CONFIG"
---

<objective>
Create a CTE performance benchmark suite and collect baseline metrics for recursive lineage queries at various depths.

Purpose: Establish baseline performance data to measure optimization impact and identify bottlenecks in recursive CTE queries.

Output:
- `database/benchmark_cte.py` - Benchmark script with configurable depth testing
- `database/benchmark_results.md` - Documented baseline results with analysis
</objective>

<execution_context>
@/Users/Daniel.Tehan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/Daniel.Tehan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@database/test_correctness.py (pattern for CTE queries)
@lineage-api/internal/adapter/outbound/teradata/openlineage_repo.go (production CTE queries)
@specs/coding_standards_sql.md (Teradata best practices)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create CTE benchmark suite</name>
  <files>database/benchmark_cte.py</files>
  <action>
Create a Python benchmark script that:

1. **Benchmark function** that times CTE execution:
   - Uses `time.perf_counter()` for high-resolution timing
   - Runs each query 3 times and reports min/avg/max times
   - Captures row counts returned at each depth

2. **Test configurations**:
   - Depths: 5, 10, 15, 20 (per PERF-CTE-01)
   - Directions: upstream, downstream
   - Test datasets: Use CHAIN_TEST (linear), FANOUT10_TEST (wide), CYCLE5_TEST (cyclic)

3. **Query patterns** (matching openlineage_repo.go):
   - Upstream query with `POSITION(lineage_id IN path) = 0` cycle detection
   - Downstream query with same pattern
   - Track path column VARCHAR size impact

4. **EXPLAIN plan capture**:
   - Run EXPLAIN for each query configuration
   - Save explain output to identify AMP usage and spool operations

5. **Output format**:
   - Print results in markdown table format
   - Show: Dataset, Direction, Depth, Min Time, Avg Time, Max Time, Rows, Path Bytes

6. **Database connection**: Import from db_config.py using existing pattern

Key implementation details:
- Use parameterized max_depth in the CTE (not hardcoded)
- VARCHAR(4000) for path column (matches Go implementation)
- Test with LOCKING ROW FOR ACCESS hint for comparison
- Handle timeout for queries exceeding 30 seconds

Reference the CTE pattern from test_correctness.py:
```python
def build_upstream_query(dataset: str, field: str, max_depth: int) -> str:
    return f"""
        WITH RECURSIVE lineage_path (...) AS (
            -- Base case
            SELECT ..., 1 AS depth, CAST(lineage_id AS VARCHAR(4000)) AS path
            FROM demo_user.OL_COLUMN_LINEAGE
            WHERE target_dataset = '{dataset}' AND target_field = '{field}'

            UNION ALL

            -- Recursive case with cycle detection
            SELECT ..., lp.depth + 1, lp.path || ',' || l.lineage_id
            FROM demo_user.OL_COLUMN_LINEAGE l
            INNER JOIN lineage_path lp ON ...
            WHERE lp.depth < {max_depth}
              AND POSITION(l.lineage_id IN lp.path) = 0
        )
        SELECT COUNT(*) AS row_count, MAX(depth) AS max_depth
        FROM lineage_path
    """
```
  </action>
  <verify>
Run `python database/benchmark_cte.py --help` to verify script is executable and shows usage.
  </verify>
  <done>
Benchmark script created with depth configuration, timing, row counting, and EXPLAIN capture functionality.
  </done>
</task>

<task type="auto">
  <name>Task 2: Run benchmarks and document baseline results</name>
  <files>database/benchmark_results.md</files>
  <action>
Execute the benchmark script and document results:

1. **Run full benchmark suite**:
   ```bash
   cd database && python benchmark_cte.py
   ```

2. **Create benchmark_results.md** with:

   a) **Benchmark Configuration** section:
      - Teradata environment details (host type: ClearScape/Production)
      - Test date
      - Test data sizes (record counts in OL_COLUMN_LINEAGE)

   b) **Baseline Results** table:
      | Dataset | Direction | Depth | Min (ms) | Avg (ms) | Max (ms) | Rows | Notes |

   c) **Performance Analysis** section:
      - Identify depth threshold where performance degrades
      - Compare linear (CHAIN) vs wide (FANOUT) patterns
      - Compare cyclic vs acyclic data

   d) **Bottleneck Identification** section (PERF-CTE-02):
      - Path concatenation overhead analysis
      - POSITION function search overhead
      - AMP distribution from EXPLAIN
      - Spool space usage observations

   e) **Baseline Conclusions**:
      - Current acceptable depth limit
      - Identified optimization candidates

3. **Key metrics to capture**:
   - Time per recursion level (if measurable)
   - Total spool bytes (from EXPLAIN)
   - AMP count involved in query
   - Path column growth rate

If database is unavailable, document that benchmarks are pending and describe the expected metrics format.
  </action>
  <verify>
Check that benchmark_results.md exists and contains:
- Results table with timing data
- Analysis of bottlenecks
- Depth limit recommendations
  </verify>
  <done>
Baseline benchmark results documented with performance analysis and identified bottlenecks ready for optimization in Plan 02.
  </done>
</task>

</tasks>

<verification>
1. `python database/benchmark_cte.py --help` shows usage
2. `python database/benchmark_cte.py` runs without errors (or gracefully handles missing DB)
3. `database/benchmark_results.md` contains baseline metrics and analysis
4. Bottlenecks identified and documented for Plan 02 optimization
</verification>

<success_criteria>
- Benchmark script is reusable for pre/post optimization comparison
- Baseline results cover depths 5, 10, 15, 20 (PERF-CTE-01)
- Performance bottlenecks identified (PERF-CTE-02)
- Clear metrics to measure optimization impact
</success_criteria>

<output>
After completion, create `.planning/phases/17-cte-performance-optimization/17-01-SUMMARY.md`
</output>
